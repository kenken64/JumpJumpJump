# PRP: 机器学习AI实现

## 问题陈述
游戏需要一个能够从人类游戏模式中学习并适应不同游戏风格的AI玩家，同时使用行为克隆和强化学习两种方法。

## 需求
- 记录玩家游戏动作和游戏状态
- 使用记录的游戏数据训练神经网络（行为克隆）
- 使用深度Q学习训练AI（强化学习）
- 使AI能够控制玩家角色
- 支持手动、规则AI、ML AI和DQN AI控制之间的切换
- 将训练好的模型持久化存储在浏览器中（IndexedDB）
- 提供带有进度反馈的训练UI
- 处理第5、10、15关等的Boss战
- 跨关卡保持录制状态

## 解决方案

### 架构概述
实现了两种AI方法：

1. **MLAIPlayer（行为克隆）**：通过模仿记录的玩家游戏来学习
2. **DQNAgent（强化学习）**：通过带奖励的试错来学习

---

## 第一部分：行为克隆（MLAIPlayer）

### 神经网络设计
- **输入层**：17个特征
  - 玩家状态：位置(x, y)、速度(x, y)、生命值、是否在地面
  - 环境：最近敌人（距离、角度）、最近金币（距离、角度）、最近尖刺（距离）
  - 导航：前方是否有地面、后方是否有地面、上方平台、上方平台高度
  - 进度：分数、金币
- **隐藏层**：128 → dropout(0.3) → 64 → dropout(0.3) → 32个神经元
- **输出层**：4个动作（左移、右移、跳跃、射击），sigmoid激活函数
- **训练**：100个epoch，批量大小16，Adam优化器（lr=0.001），二元交叉熵损失

### 关键控制
- **R键**：切换录制（存储在localStorage）
- **O键**：切换ML AI控制

---

## 第二部分：深度Q网络（DQNAgent）- 新功能

### DQN架构
- **状态空间**：14维
  - `playerX`、`playerY`：归一化的玩家位置
  - `velocityX`、`velocityY`：玩家速度
  - `onGround`：布尔值（是否在地面）
  - `nearestPlatformX`、`nearestPlatformY`：到最近平台的距离
  - `nearestEnemyX`、`nearestEnemyY`：到最近敌人的距离
  - `nearestSpikeX`：到最近尖刺的距离
  - `bossActive`：布尔值（Boss战是否激活）
  - `bossDistance`：到Boss的距离
  - `bossHealth`：Boss生命值百分比

- **动作空间**：9个离散动作
  - 0：待机
  - 1：左移
  - 2：右移
  - 3：跳跃
  - 4：左移 + 跳跃
  - 5：右移 + 跳跃
  - 6：射击
  - 7：右移 + 射击
  - 8：右移 + 跳跃 + 射击

- **网络架构**：
  - 输入：14个神经元
  - 隐藏层：128 → 128 → 64个神经元（ReLU激活函数）
  - 输出：9个神经元（每个动作的Q值）
  - 目标网络，软更新（τ = 0.01）

### DQN超参数
```typescript
learningRate: 0.0005
gamma: 0.99  // 折扣因子
epsilon: 1.0 → 0.1  // 探索率衰减
epsilonDecay: 0.999  // 保持更多探索
batchSize: 64
replayBufferSize: 50000
minReplaySize: 500
targetUpdateFrequency: 100
```

### 奖励塑造
1. **进度奖励**：
   - 向前进度：每向右移动一个单位+0.1
   - 到达新的最大X位置：+0.5奖励
   - 垂直攀爬：向上移动+0.3

2. **生存奖励**：
   - 保持存活：每帧+0.05
   - 在地面上：+0.02

3. **战斗奖励**：
   - 击杀敌人：每次+5.0
   - 向敌人射击（Boss激活时）：+0.8

4. **Boss交战**（第5、10、15关...）：
   - 接近Boss：+0.5接近奖励
   - Boss存活时尝试进入传送门：-2.0惩罚
   - Boss未被击败时传送门被阻挡

5. **卡住检测和惩罚**：
   - 超激进检测（5帧阈值触发撤退）
   - 撤退行为：向左移动并尝试二段跳
   - 卡住90帧后强制随机探索
   - 渐进惩罚：根据卡住时长-0.1到-0.8

### 模仿学习（人类示范）
DQN可以从记录的人类游戏中学习：

1. **录制**（T键）：
   - 切换录制开/关
   - 每3帧采样一次以减少数据
   - 压缩状态数据以避免localStorage配额
   - 录制跨关卡持续

2. **数据压缩**：
   - 短属性名（`playerX` → `px`）
   - 四舍五入浮点数（2位小数）
   - 布尔值转为0/1
   - 最多存储5000帧

3. **导入**（I键）：
   - 解压保存的示范
   - 以+0.5奖励提升添加到回放缓冲区
   - 立即使用导入的数据训练

### DQN训练控制
| 按键 | 动作 |
|-----|--------|
| **A** | 切换DQN AI控制（游戏内） |
| **T** | 切换录制玩家游戏 |
| **I** | 导入录制的示范 |
| **空格** | 暂停/继续训练 |
| **R** | 重置当前回合 |
| **S** | 保存模型到IndexedDB |
| **L** | 加载保存的模型 |
| **1-5** | 设置游戏速度（1x-5x） |
| **ESC** | 退出到菜单 |

### DQN训练场景
专用训练场景（`DQNTrainingScene`）提供：
- 训练期间的实时游戏预览
- 回合/步数计数器
- 奖励历史图表
- Epsilon（探索率）显示
- 模型保存/加载状态

---

## 实现文件

### DQN文件
1. **frontend/src/utils/DQNAgent.ts**（约530行）
   - DQNState接口（14维）
   - 神经网络构建（策略网络+目标网络）
   - 经验回放缓冲区
   - 带卡住检测的奖励计算
   - 模仿学习方法
   - 模型持久化（IndexedDB）

2. **frontend/src/scenes/DQNTrainingScene.ts**（约360行）
   - 训练UI覆盖层
   - 回合管理
   - 奖励可视化

3. **frontend/src/scenes/GameScene.ts**（DQN集成）
   - `extractDQNState()`：构建14维状态
   - `handleDQNKeyboardControls()`：训练控制
   - `handleDQNRecordingKey()`：T/I键处理器
   - `recordPlayerFrame()`：捕获示范
   - `importDemonstrationsToDQN()`：加载保存的数据
   - Boss检测和传送门阻挡

### ML AI文件（已有）
1. **frontend/src/utils/GameplayRecorder.ts**：记录17特征状态
2. **frontend/src/utils/MLAIPlayer.ts**：行为克隆网络

---

## 游戏流程集成

### 关卡过渡数据
过渡到下一关时，以下状态被保留：
```typescript
{
  gameMode: 'levels',
  level: currentLevel + 1,
  lives: playerLives,      // 跨关卡保留
  score: score,            // 累积分数
  isRecording: isRecordingForDQN,  // 录制继续
  mode: 'coop',            // 如果是合作模式
  dqnTraining: true        // 如果是DQN训练模式
}
```

### Boss战（第5、10、15关...）
- Boss在关卡末尾生成
- Boss未被击败时传送门被阻挡
- DQN战斗时获得交战奖励
- Boss存活时DQN接近传送门会受惩罚

---

## 测试与验证

### DQN训练
1. 启动游戏 → 按A进入DQN模式
2. 按空格开始训练
3. 观察AI学习（epsilon随时间降低）
4. 按S保存模型
5. 刷新 → 按L加载

### 模仿学习
1. 正常玩游戏
2. 按T开始录制
3. 录制时完成1-3关
4. 按T停止录制
5. 按I导入到DQN
6. AI根据你的示范训练

### Boss交战
1. 训练DQN到第5关
2. 验证DQN战斗Boss而不是逃跑
3. 验证Boss死亡前传送门被阻挡

---

## 已知问题与解决方案

1. **模型状态不匹配**：旧的11维模型与新的14维状态不兼容
   - 解决方案：自动检测并删除不兼容的模型

2. **localStorage配额**：录制太多帧超过5MB限制
   - 解决方案：压缩数据，限制5000帧，每3帧采样一次

3. **文本渲染错误**：场景过渡时录制状态文本被销毁
   - 解决方案：setText调用周围添加空检查和try-catch

4. **AI卡住**：AI无限重复相同动作
   - 解决方案：带撤退行为的超激进卡住检测

---

## 性能指标

- **DQN训练**：每步约10ms（游戏运行时）
- **推理时间**：每次决策<5ms
- **模型大小**：IndexedDB中约200KB
- **录制存储**：每1000压缩帧约50KB

---

## 状态
✅ **已完成**：
- MLAIPlayer行为克隆（17特征）
- DQNAgent强化学习（14维，9个动作）
- 从人类示范的模仿学习
- Boss交战系统
- 跨关卡录制持续
- 超激进卡住检测

🔄 **未来改进**：
- 优先经验回放
- 对决DQN架构
- 多智能体训练
- 课程学习（简单→困难关卡）
